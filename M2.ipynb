{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd51df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, RDD\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import col,split,countDistinct,min\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367b2e7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edee3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab02616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_flatmap(x):\n",
    "    elements = x.split(\",\")\n",
    "    attr = elements[1].split(';')\n",
    "    num = elements[2].split(';')\n",
    "    l = []\n",
    "\n",
    "    for i in range(0,5):\n",
    "        l.append(elements[0] + ',' + attr[i] + ',' + num[i])\n",
    "\n",
    "    \n",
    "    return l\n",
    "        \n",
    "        \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd6cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_context(on_server) -> SparkContext:\n",
    "    spark_conf = SparkConf().setAppName(\"2ID70-MS2\")\n",
    "    if not on_server:\n",
    "        spark_conf = spark_conf.setMaster(\"local[*]\")\n",
    "    return SparkContext.getOrCreate(spark_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bf5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(spark_context: SparkContext, on_server) -> RDD:\n",
    "    database_file_path = \"/Database.csv\" if on_server else \"Database.csv\"\n",
    "\n",
    "    # TODO: You may change the value for the minPartitions parameter (template value 160) when running locally.\n",
    "    # It is advised (but not compulsory) to set the value to 160 when running on the server.\n",
    "    database_rdd = spark_context.textFile(database_file_path, 160)\n",
    "    \n",
    "    # TODO: Implement Q1 here by defining q1RDD based on databaseRDD.\n",
    "    q1_rdd = None\n",
    "\n",
    "    return q1_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc105d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"2ID70-MS2\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb4ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_rdd = sc.textFile(\"C:/Users/20190938/Desktop/m2/Database.csv\", 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a7695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = database_rdd.first()\n",
    "q1_rdd = database_rdd\\\n",
    "    .filter(lambda x: x != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042f9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_rdd = q1_rdd.flatMap(lambda x: row_flatmap(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72df68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A,c,19619',\n",
       " 'A,q,33917',\n",
       " 'A,s,0',\n",
       " 'A,u,117',\n",
       " 'A,z,48337',\n",
       " 'A,c,31260',\n",
       " 'A,q,28358',\n",
       " 'A,s,11',\n",
       " 'A,u,43',\n",
       " 'A,z,22051']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ba9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1s_rdd = q1_rdd.filter(lambda x: x[0] == 'S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac798bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = q1s_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be4b139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1t_rdd = q1_rdd.filter(lambda x: x[0] == 'T')\n",
    "\n",
    "t = q1t_rdd.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0be0615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25133455",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1r_rdd = q1_rdd.filter(lambda x: x[0] == 'R')\n",
    "\n",
    "r =  q1r_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c973ad82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> [q1: R: 250000]\n",
      ">> [q1: S: 250000]\n",
      ">> [q1: T: 250000]\n"
     ]
    }
   ],
   "source": [
    "print(\">> [q1: R: \" + str(r) + \"]\")\n",
    "print(\">> [q1: S: \" + str(s) + \"]\")\n",
    "print(\">> [q1: T: \" + str(t) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd98ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dc23588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2(spark_context: SparkContext, q1_rdd: RDD):\n",
    "    spark_session = SparkSession(spark_context)\n",
    "\n",
    "    # TODO: Implement Q2 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365fd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1 = SparkContext.getOrCreate(conf)\n",
    "\n",
    "spark_session = SparkSession(sc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8817c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = q1_rdd.map(lambda r:Row(r)).toDF([\"RelationName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5794f77b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'RelationName'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-244236c7ce17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRelationName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'no'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1660\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'RelationName'"
     ]
    }
   ],
   "source": [
    "data_frame = data_frame.select(split(data_frame.RelationName,\",\")).rdd.flatMap(lambda x: x).toDF(schema =['rel','name','no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "327d865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.filter(col(\"rel\") == 'R').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4738cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "q22 = data_frame.groupBy([\"name\"]).agg(countDistinct(\"no\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eb0133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q23 = q22.filter(col(\"count(no)\")>=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f649a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q23.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8205c741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(count(no))=493)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q22.agg(min(col(\"count(no)\"))).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b03e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3(spark_context: SparkContext, q1_rdd: RDD):\n",
    "    # TODO: Implement Q3 here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4(spark_context: SparkContext, on_server):\n",
    "    streaming_context = StreamingContext(spark_context, 2)\n",
    "    streaming_context.checkpoint(\"checkpoint\")\n",
    "\n",
    "    hostname = \"stream-host\" if on_server else \"localhost\"\n",
    "    lines = streaming_context.socketTextStream(hostname, 9000)\n",
    "\n",
    "    # TODO: Implement Q4 here.\n",
    "\n",
    "    # Start the streaming context, run it for two minutes or until termination\n",
    "    streaming_context.start()\n",
    "    streaming_context.awaitTerminationOrTimeout(2 * 60)\n",
    "    streaming_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a60d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main 'function' which initializes a Spark context and runs the code for each question.\n",
    "# To skip executing a question while developing a solution, simply comment out the corresponding function call.\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    on_server = False  # TODO: Set this to true if and only if running on the server\n",
    "\n",
    "    spark_context = get_spark_context(on_server)\n",
    "\n",
    "    q1_rdd = q1(spark_context, on_server)\n",
    "\n",
    "    q2(spark_context, q1_rdd)\n",
    "\n",
    "    q3(spark_context, q1_rdd)\n",
    "\n",
    "    q4(spark_context, on_server)\n",
    "\n",
    "    spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359ea59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63d98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
